{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Heart Failure Simple DNN Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=qwiklabs-asl-02-99f66d8df225\n",
      "env: BUCKET=qwiklabs-asl-02-99f66d8df225\n",
      "env: REGION=us-central1\n"
     ]
    }
   ],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "%env PROJECT = {PROJECT}\n",
    "%env BUCKET = {BUCKET}\n",
    "%env REGION = {REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipelines/kfp_heart_failure_simple_dnn_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipelines/kfp_heart_failure_simple_dnn_pipeline.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"Kubeflow Simple DNN Heart Failure Pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import (\n",
    "    CustomTrainingJobOp,\n",
    ")\n",
    "\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    AutoMLTabularTrainingJobRunOp,\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    TabularDatasetCreateOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT = os.getenv(\"PROJECT\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "DATASET_URI = os.getenv(\"DATASET_URI\")\n",
    "OUTPUT_URI = os.getenv(\"OUTPUT_URI\")\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"kfp-heartfailure-simple-dnn\")\n",
    "DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-4\")\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "BASE_OUTPUT_DIR = os.getenv(\"BASE_OUTPUT_DIR\")\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-vertex-pipeline\",\n",
    "    description=f\"Vertex Pipeline for {PIPELINE_NAME}\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def create_pipeline():\n",
    "\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRAINING_CONTAINER_IMAGE_URI,\n",
    "                \"args\": [\n",
    "                    f\"--dataset_uri={DATASET_URI}\",\n",
    "                    f\"--output_uri={OUTPUT_URI}\",\n",
    "                    \"--epochs=100\",\n",
    "                    \"--batch_size=100\",\n",
    "                    \"--lr=.001\",\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    training_task = CustomTrainingJobOp(\n",
    "        project=PROJECT,\n",
    "        location=REGION,\n",
    "        display_name=f\"{PIPELINE_NAME}-training-job\",\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=BASE_OUTPUT_DIR,\n",
    "    )\n",
    "    \n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project=PROJECT,\n",
    "        display_name=f\"{PIPELINE_NAME}-upload-job\",\n",
    "        #artifact_uri=OUTPUT_URI,\n",
    "        artifact_uri=f\"{BASE_OUTPUT_DIR}/model\",\n",
    "        serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    ) \n",
    "    model_upload_task.after(training_task)\n",
    "\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT,\n",
    "        display_name=f\"{PIPELINE_NAME}-endpoint-job\",\n",
    "        description=\"Heart Failure Simple DNN model\",\n",
    "    )\n",
    "    endpoint_create_task.after(model_upload_task)\n",
    "\n",
    "    model_deploy_task = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        #enable_access_logging=True, #comment out because of failure\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-asl-02-99f66d8df225-kfp-simple-dnn-artifact-store/simple-dnn-pipeline\n",
      "env: PROJECT=qwiklabs-asl-02-99f66d8df225\n",
      "env: REGION=us-central1\n",
      "env: DATASET_URI=gs://qwiklabs-asl-02-99f66d8df225/heart_failure/scaled-engineered-heart.csv\n",
      "env: OUTPUT_URI=gs://qwiklabs-asl-02-99f66d8df225/heart_failure/20230208225130\n",
      "env: BASE_OUTPUT_DIR=gs://qwiklabs-asl-02-99f66d8df225-kfp-simple-dnn-artifact-store/models/20230208225130\n",
      "gs://qwiklabs-asl-02-99f66d8df225-kfp-simple-dnn-artifact-store/models/20230208225130\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT}-kfp-simple-dnn-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/simple-dnn-pipeline\"\n",
    "DATASET_URI = f\"gs://{BUCKET}/heart_failure/scaled-engineered-heart.csv\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BASE_OUTPUT_DIR = f\"{ARTIFACT_STORE}/models/{TIMESTAMP}\"\n",
    "OUTPUT_URI = f\"gs://{BUCKET}/heart_failure/{TIMESTAMP}\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT={PROJECT}\n",
    "%env REGION={REGION}\n",
    "%env DATASET_URI={DATASET_URI}\n",
    "%env OUTPUT_URI={OUTPUT_URI}\n",
    "%env BASE_OUTPUT_DIR={BASE_OUTPUT_DIR}\n",
    "\n",
    "!echo {BASE_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-02-99f66d8df225-kfp-simple-dnn-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/heart-failure\n",
      "total 24K\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Feb  8 17:40 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 jupyter jupyter  180 Feb  8 17:46 Dockerfile\n",
      "drwxr-xr-x 3 jupyter jupyter 4.0K Feb  8 17:46 .\n",
      "drwxr-xr-x 6 jupyter jupyter 4.0K Feb  8 22:48 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 6.8K Feb  8 22:51 train.py\n",
      "FROM gcr.io/deeplearning-platform-release/tf-cpu.2-8\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!cp ./pipelines/train.py ./trainer_image_vertex/\n",
    "!ls -altrh ./trainer_image_vertex\n",
    "!cat ./trainer_image_vertex/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-02-99f66d8df225/trainer_image_heart_failure_simple_dnn_vertex:latest'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_heart_failure_simple_dnn_vertex\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the container via cloud build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 7.1 KiB before compression.\n",
      "Uploading tarball of [trainer_image_vertex] to [gs://qwiklabs-asl-02-99f66d8df225_cloudbuild/source/1675878461.278595-6334ce48cbe7453e990fc0eff6de4308.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-02-99f66d8df225/locations/global/builds/2a964443-6e75-4feb-9fc8-2a94615655e8].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/2a964443-6e75-4feb-9fc8-2a94615655e8?project=9475810701 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2a964443-6e75-4feb-9fc8-2a94615655e8\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-02-99f66d8df225_cloudbuild/source/1675878461.278595-6334ce48cbe7453e990fc0eff6de4308.tgz#1675878461576078\n",
      "Copying gs://qwiklabs-asl-02-99f66d8df225_cloudbuild/source/1675878461.278595-6334ce48cbe7453e990fc0eff6de4308.tgz#1675878461576078...\n",
      "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
      "Operation completed over 1 objects/3.0 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.26kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf-cpu.2-8\n",
      "latest: Pulling from deeplearning-platform-release/tf-cpu.2-8\n",
      "846c0b181fff: Pulling fs layer\n",
      "0262b90c827a: Pulling fs layer\n",
      "94582730ee25: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "fdbfba2d28ed: Pulling fs layer\n",
      "57659115920f: Pulling fs layer\n",
      "105e94dcdb8c: Pulling fs layer\n",
      "1dfd2abf66a4: Pulling fs layer\n",
      "3202f2efe470: Pulling fs layer\n",
      "de0c8b77fdff: Pulling fs layer\n",
      "e5414ff3d65b: Pulling fs layer\n",
      "cdf5a5b77313: Pulling fs layer\n",
      "65deab6bafc4: Pulling fs layer\n",
      "e95a164d5c12: Pulling fs layer\n",
      "bba688a39323: Pulling fs layer\n",
      "171a421901cd: Pulling fs layer\n",
      "fa249cf147f5: Pulling fs layer\n",
      "194428869cce: Pulling fs layer\n",
      "afc32a09a27d: Pulling fs layer\n",
      "43b59cae3eea: Pulling fs layer\n",
      "ada9fc7c6e60: Pulling fs layer\n",
      "5c560168862c: Pulling fs layer\n",
      "aee3d57508ee: Pulling fs layer\n",
      "d2392b09e421: Pulling fs layer\n",
      "247264149272: Pulling fs layer\n",
      "1dfd2abf66a4: Waiting\n",
      "3202f2efe470: Waiting\n",
      "de0c8b77fdff: Waiting\n",
      "e5414ff3d65b: Waiting\n",
      "cdf5a5b77313: Waiting\n",
      "65deab6bafc4: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "fdbfba2d28ed: Waiting\n",
      "57659115920f: Waiting\n",
      "e95a164d5c12: Waiting\n",
      "bba688a39323: Waiting\n",
      "105e94dcdb8c: Waiting\n",
      "171a421901cd: Waiting\n",
      "fa249cf147f5: Waiting\n",
      "194428869cce: Waiting\n",
      "43b59cae3eea: Waiting\n",
      "ada9fc7c6e60: Waiting\n",
      "5c560168862c: Waiting\n",
      "aee3d57508ee: Waiting\n",
      "d2392b09e421: Waiting\n",
      "247264149272: Waiting\n",
      "94582730ee25: Verifying Checksum\n",
      "94582730ee25: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "846c0b181fff: Verifying Checksum\n",
      "846c0b181fff: Download complete\n",
      "0262b90c827a: Verifying Checksum\n",
      "0262b90c827a: Download complete\n",
      "105e94dcdb8c: Verifying Checksum\n",
      "105e94dcdb8c: Download complete\n",
      "1dfd2abf66a4: Verifying Checksum\n",
      "1dfd2abf66a4: Download complete\n",
      "3202f2efe470: Verifying Checksum\n",
      "3202f2efe470: Download complete\n",
      "57659115920f: Verifying Checksum\n",
      "57659115920f: Download complete\n",
      "e5414ff3d65b: Verifying Checksum\n",
      "e5414ff3d65b: Download complete\n",
      "cdf5a5b77313: Verifying Checksum\n",
      "cdf5a5b77313: Download complete\n",
      "65deab6bafc4: Verifying Checksum\n",
      "65deab6bafc4: Download complete\n",
      "e95a164d5c12: Verifying Checksum\n",
      "e95a164d5c12: Download complete\n",
      "bba688a39323: Verifying Checksum\n",
      "bba688a39323: Download complete\n",
      "171a421901cd: Download complete\n",
      "de0c8b77fdff: Verifying Checksum\n",
      "de0c8b77fdff: Download complete\n",
      "194428869cce: Verifying Checksum\n",
      "194428869cce: Download complete\n",
      "fa249cf147f5: Verifying Checksum\n",
      "fa249cf147f5: Download complete\n",
      "afc32a09a27d: Verifying Checksum\n",
      "afc32a09a27d: Download complete\n",
      "43b59cae3eea: Verifying Checksum\n",
      "43b59cae3eea: Download complete\n",
      "fdbfba2d28ed: Verifying Checksum\n",
      "fdbfba2d28ed: Download complete\n",
      "5c560168862c: Verifying Checksum\n",
      "5c560168862c: Download complete\n",
      "d2392b09e421: Download complete\n",
      "247264149272: Verifying Checksum\n",
      "247264149272: Download complete\n",
      "aee3d57508ee: Verifying Checksum\n",
      "aee3d57508ee: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "0262b90c827a: Pull complete\n",
      "94582730ee25: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ada9fc7c6e60: Download complete\n",
      "fdbfba2d28ed: Pull complete\n",
      "57659115920f: Pull complete\n",
      "105e94dcdb8c: Pull complete\n",
      "1dfd2abf66a4: Pull complete\n",
      "3202f2efe470: Pull complete\n",
      "de0c8b77fdff: Pull complete\n",
      "e5414ff3d65b: Pull complete\n",
      "cdf5a5b77313: Pull complete\n",
      "65deab6bafc4: Pull complete\n",
      "e95a164d5c12: Pull complete\n",
      "bba688a39323: Pull complete\n",
      "171a421901cd: Pull complete\n",
      "fa249cf147f5: Pull complete\n",
      "194428869cce: Pull complete\n",
      "afc32a09a27d: Pull complete\n",
      "43b59cae3eea: Pull complete\n",
      "ada9fc7c6e60: Pull complete\n",
      "5c560168862c: Pull complete\n",
      "aee3d57508ee: Pull complete\n",
      "d2392b09e421: Pull complete\n",
      "247264149272: Pull complete\n",
      "Digest: sha256:00a965f0a144a0fade85aa37dc87b82ffe0493846c70c2bd646940a8f731d5fc\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf-cpu.2-8:latest\n",
      " ---> b47380937db7\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4\n",
      " ---> Running in 8a4ba5ee5947\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 3.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 20.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire) (2.2.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=bd0b67ccc47fec420a8802e7ba1958416c68bd93d91438e6c5b8c8da1572097d\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/e8/7b/003fc14f02f262dd4614aec55e41147c8012e3dad98c936b76\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=bb4c4cc4404da4d269f91ebbf87f81b190b58873b30e6972c44b62d9dd5ffb58\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/fb/ed/cfc98e70373dfe12db85fffab293e3153162f63de2f6aa5473\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, fire, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.5.0 scikit-learn-0.20.4\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 8a4ba5ee5947\n",
      " ---> fa66edee6e72\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in dc07f511b448\n",
      "Removing intermediate container dc07f511b448\n",
      " ---> 19cdc2573095\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> f5ed3605b450\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in eaa3fc0f9612\n",
      "Removing intermediate container eaa3fc0f9612\n",
      " ---> 0521013ad45f\n",
      "Successfully built 0521013ad45f\n",
      "Successfully tagged gcr.io/qwiklabs-asl-02-99f66d8df225/trainer_image_heart_failure_simple_dnn_vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-02-99f66d8df225/trainer_image_heart_failure_simple_dnn_vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-02-99f66d8df225/trainer_image_heart_failure_simple_dnn_vertex]\n",
      "b3f5612747b4: Preparing\n",
      "39695a9ea03a: Preparing\n",
      "5f48db77ec3b: Preparing\n",
      "5e2759673611: Preparing\n",
      "037b41a5f8c2: Preparing\n",
      "bb92a6628c1b: Preparing\n",
      "8e128f53805f: Preparing\n",
      "705ba961b4b6: Preparing\n",
      "9bd45ad48372: Preparing\n",
      "f7e5a23f26c7: Preparing\n",
      "c8d0f8950988: Preparing\n",
      "17f4d49e1816: Preparing\n",
      "687cd89a0b51: Preparing\n",
      "5280a42c1dce: Preparing\n",
      "5fd67125a305: Preparing\n",
      "308ed959addf: Preparing\n",
      "bcb419782cbc: Preparing\n",
      "80af522d481e: Preparing\n",
      "7eea273b6e49: Preparing\n",
      "2e3f4a474da9: Preparing\n",
      "bf78c9e497c6: Preparing\n",
      "ba904aab1189: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1feabd1a8af4: Preparing\n",
      "911cdb57b88b: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "b610f5fa095b: Preparing\n",
      "60ffaefa78fb: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "bb92a6628c1b: Waiting\n",
      "8e128f53805f: Waiting\n",
      "705ba961b4b6: Waiting\n",
      "9bd45ad48372: Waiting\n",
      "f7e5a23f26c7: Waiting\n",
      "c8d0f8950988: Waiting\n",
      "17f4d49e1816: Waiting\n",
      "687cd89a0b51: Waiting\n",
      "5280a42c1dce: Waiting\n",
      "5fd67125a305: Waiting\n",
      "308ed959addf: Waiting\n",
      "bcb419782cbc: Waiting\n",
      "80af522d481e: Waiting\n",
      "7eea273b6e49: Waiting\n",
      "2e3f4a474da9: Waiting\n",
      "bf78c9e497c6: Waiting\n",
      "ba904aab1189: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "1feabd1a8af4: Waiting\n",
      "911cdb57b88b: Waiting\n",
      "b610f5fa095b: Waiting\n",
      "60ffaefa78fb: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "5e2759673611: Mounted from deeplearning-platform-release/tf-cpu.2-8\n",
      "037b41a5f8c2: Mounted from deeplearning-platform-release/tf-cpu.2-8\n",
      "bb92a6628c1b: Mounted from deeplearning-platform-release/tf-cpu.2-8\n",
      "8e128f53805f: Mounted from deeplearning-platform-release/tf-cpu.2-8\n",
      "b3f5612747b4: Pushed\n",
      "9bd45ad48372: Layer already exists\n",
      "39695a9ea03a: Pushed\n",
      "705ba961b4b6: Mounted from deeplearning-platform-release/tf-cpu.2-8\n",
      "c8d0f8950988: Layer already exists\n",
      "17f4d49e1816: Layer already exists\n",
      "f7e5a23f26c7: Layer already exists\n",
      "5280a42c1dce: Layer already exists\n",
      "687cd89a0b51: Layer already exists\n",
      "5fd67125a305: Layer already exists\n",
      "308ed959addf: Layer already exists\n",
      "bcb419782cbc: Layer already exists\n",
      "7eea273b6e49: Layer already exists\n",
      "bf78c9e497c6: Layer already exists\n",
      "80af522d481e: Layer already exists\n",
      "2e3f4a474da9: Layer already exists\n",
      "ba904aab1189: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "1feabd1a8af4: Layer already exists\n",
      "911cdb57b88b: Layer already exists\n",
      "b610f5fa095b: Layer already exists\n",
      "60ffaefa78fb: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "5f48db77ec3b: Pushed\n",
      "latest: digest: sha256:7c1eb90b09b556a5e820f42ba5bd4a400f7fdd126abd870858ffe274f28e48c5 size: 6391\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                                       STATUS\n",
      "2a964443-6e75-4feb-9fc8-2a94615655e8  2023-02-08T17:47:41+00:00  4M42S     gs://qwiklabs-asl-02-99f66d8df225_cloudbuild/source/1675878461.278595-6334ce48cbe7453e990fc0eff6de4308.tgz  gcr.io/qwiklabs-asl-02-99f66d8df225/trainer_image_heart_failure_simple_dnn_vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI trainer_image_vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the training and serving URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-asl-02-99f66d8df225/trainer_image_heart_failure_simple_dnn_vertex:latest\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\n"
     ]
    }
   ],
   "source": [
    "#SERVING_CONTAINER_IMAGE_URI = \"gcr.io/deeplearning-platform-release/tf-cpu.2-8\"\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the pipeline from the Python file we generated into a JSON description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"pipelines/kfp_heart_failure_simple_dnn_pipeline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py pipelines/kfp_heart_failure_simple_dnn_pipeline.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline:\n",
    "\n",
    "```python\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_JSON,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-custom-training-job\": {\n",
      "        \"executorLabel\": \"exec-custom-training-job\",\n",
      "        \"inputDefinitions\": {\n",
      "          \"parameters\": {\n",
      "            \"base_output_directory\": {\n",
      "              \"type\": \"STRING\"\n",
      "            },\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions for class:\n",
    "\n",
    "I hit this error - The replica workerpool0-0 exited with a non-zero status of 13. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=9475810701&resource=ml_job%2Fjob_id%2F3079020273060544512&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%223079020273060544512%22\n",
    "\n",
    "- how do I restart a pipeline at a failed step?\n",
    "- How do I get permissions to view this log - https://console.cloud.google.com/logs/viewer?project=9475810701&resource=ml_job%2Fjob_id%2F3079020273060544512&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%223079020273060544512%22\n",
    "- How do I debug this error: The replica workerpool0-0 exited with a non-zero status of 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/9475810701/locations/us-central1/pipelineJobs/kfp-heartfailure-simple-dnn-vertex-pipeline-20230209030201\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/9475810701/locations/us-central1/pipelineJobs/kfp-heartfailure-simple-dnn-vertex-pipeline-20230209030201')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kfp-heartfailure-simple-dnn-vertex-pipeline-20230209030201?project=9475810701\n",
      "PipelineJob projects/9475810701/locations/us-central1/pipelineJobs/kfp-heartfailure-simple-dnn-vertex-pipeline-20230209030201 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/9475810701/locations/us-central1/pipelineJobs/kfp-heartfailure-simple-dnn-vertex-pipeline-20230209030201 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/9475810701/locations/us-central1/pipelineJobs/kfp-heartfailure-simple-dnn-vertex-pipeline-20230209030201 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"kfp_heart_failure_simple_dnn_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
